{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"contact_us/","text":"Contact US \u00b6 Need any more support feel free to contact us!! Contact No. \u00b6 +919633964140 Email \u00b6 ccmsadmin@nitc.ac.in Address \u00b6 Centre for Computaional Modelling & Simulation, National Institute of Technology Calicut, NIT Campus P.O 673 601, Kozhikode, India","title":"Contact Us"},{"location":"contact_us/#contact-us","text":"Need any more support feel free to contact us!!","title":"Contact US"},{"location":"contact_us/#contact-no","text":"+919633964140","title":"Contact No."},{"location":"contact_us/#email","text":"ccmsadmin@nitc.ac.in","title":"Email"},{"location":"contact_us/#address","text":"Centre for Computaional Modelling & Simulation, National Institute of Technology Calicut, NIT Campus P.O 673 601, Kozhikode, India","title":"Address"},{"location":"getting-started/","text":"Introduction \u00b6 Centre for Computational Modelling and Simulation (CCMS) aims to promote and support computational modelling as a mainstream research activity amongst the faculty and researchers of NIT Calicut. A High Performance Computing (HPC) Cluster and NVIDIA DGX Station purchased under HEFA are the major computing facility under this centre. The facility consists of 31 CPU nodes and 2 GPU nodes. The NVIDIA DGX Station has four V100 GPU accelerators. The facility is mainly for developing and running parallel codes for research purposes. The HPC system installed has been ranked as the 38 th in the list of top 100 computing machines in India, in January 2021, a list maintained by CDAC, Bangalore click here . System Configuration \u00b6 Wanted to know about system configuration \ud83d\ude00? Check the it here Get an Account for You \u00b6 Centre for computational Modelling and Simulation (CCMS) comprises of a High Performance Computing cluster and a DGX Station that are available to both faculty and students. Inorder to get an account you may download the application and submit filled application to the mail id : ccmsadmin@nitc.ac.in or through hardcopy You will be notified once your account is approved. For NITC students, recommendation from the guide/ faculty is mandatory. A short proposal describing the computing to be carried out, justification for the use of HPC facility and expected outcome (1 page only) is to be attached with the submitted form. System Access \u00b6 The cluster can be accessed through Master node, which allows users to login, to submit jobs, transfer data and to compile source code. (If your compilation takes more than a few minutes, you should submit the compilation job into the queue to be run on the cluster.) By default, a user will have access to home directory (/gpfs-home/) This directory is available on the login node as well as the other nodes on the cluster. And the /gpfs-scratch/ directory may be used for temporary data storage, generally used to store data required for running jobs. Any data stored in /gpfs-scratch will be deleted after 30 days. Remote Access \u00b6 Using ssh in Windows \u00b6 Windows systems do not have any built-in support for using SSH, so you will have to download a software package to use SSH. PuTTY is the most popular open source (ie free) SSH client for Windows. To install it, visit the download site, and download the Installer package. Once installed, find the PuTTy application shortcut in your Start Menu, desktop. On clicking the PuTTy icon The PuTTy Configuration dialog should appear: Locate the Host Name input box in the PuTTy Configuration screen. Enter the server name you wish to connect to (e.g. [username]@[hpcipaddress]), and click Open. Enter your password when prompted, and press Enter. You are now connected! Using ssh in Mac or Linux \u00b6 Both Mac and Linux systems provide a built-in SSH client, so there is no need to install an additional package. Simply locate and run the Terminal app. Once in the terminal, you can connect to an SSH server by typing the following command: // to access from your pc user@my-pc:~$ ssh username@hostid // if a remote GUI section is required user@my-pc:~$ ssh username@hostid -X Note Your user name and the hostid will be recieved through mail. Please check that. and replace username & hostid with appropriate value How to change the user password? Use the \"passwd\" command to change the password for the user from the login node. Usage Policy and Guidelines \u00b6 Application software available in the system / installed by user is to be used for academic purpose only and cannot be used for the monetary benefit of an individual or a company. To protect the security of the system, the user should neither provide his/her password nor allow other individuals to use his/her account. The system administrators may verify the above fact at any point of time. If found guilty, the user id will be cancelled without further reference to the user. Individuals who attempt to use accounts, files, system resources or other facilities without authorization or those who aid other individuals doing so, may be committing a criminal act and may be subjected to criminal prosecution. It is the responsibility of each individual user to know what effects the use of certain programs and/or facilities can have on other users and/or facilities, whether it may damage system resources or severely inconvenience other users currently using the system. System files and other application software installed by users and provided under license are not to be copied or tampered with. A Project Report is to be submitted at the end of the project.(1 page max.) Acknowledgement of the use of the center\u2019s facilities should be made in journal publications, dissertations, theses, conference publications and reports published by the users. Any outcomes of the project, in terms of publications (journal / conference proceedings etc.) should be communicated to the center. Student Accounts will be deleted and the user files removed on graduation. Best Practices of HPC \u00b6 Do not go over your storage quota. Exceeding your storage quota can lead to many problems including batch jobs failing, confusing error messages and the inability to use X11 forwarding. Be sure to routinely run the \"du -sh ~/ \" command to check your usage. If more space is needed then remove files. Do not run jobs on the Master node. When you connect to a cluster via SSH you land on the Master node which is shared by all users. The login node is reserved for submitting jobs, compiling codes, installing software and running short tests that use only a few CPU-cores and finish within a few minutes. Anything more intensive must be submitted to the Slurm job scheduler as either a batch or interactive job. Failure to comply with this rule may result in your account being suspended. Do not try to access the Internet from batch or interactive jobs. All jobs submitted to the Slurm job scheduler run on the compute nodes which do not have Internet access. Because of this, a running job cannot download files, install packages or connect to GitHub. You will need to perform these operations on the login node before submitting the job. Do not allocate more than one CPU-core for serial jobs. Serial codes cannot run in parallel so using more than one CPU-core will not cause the job to run faster. Instead, doing so will waste resources. See the Slurm page for tips on figuring out if your code can run in parallel and for information about Job Arrays which allow one to run many jobs simultaneously. Do not run jobs with a parallel code without first conducting a scaling analysis. If your code runs in parallel then you need to determine the optimal number of nodes and CPU-cores to use. The same is true if it can use multiple GPUs. To do this, perform a scaling analysis as described in Choosing the Number of Nodes, CPU-cores and GPUs. Do not request a GPU for a code that can only use CPUs. Only codes that have been explicitly written to use GPUs can take advantage of GPUs. Allocating a GPU for a CPU-only code will not speed-up the execution time but it will increase your queue time, waste resources. Furthermore, some codes are written to use only a single GPU. For more see GPU Computing and Choosing the Number of Nodes, CPU-cores and GPUs. Do not load environment modules using only the partial name. You should always specify the full name of the environment module (e.g., module load compilers/intel/parallel_studio_xe_2020.4.912) and on some clusters failing to do so will result in an error. Also, you should avoid loading environment modules in your ~/.bashrc file. Instead, do this in Slurm scripts and on the command line when needed. Please do not use spaces while creating the directories and files. Transferring files between local machine and HPC cluster \u00b6 Users need to have the data and application related to their project/research work on Madhava HPC Cluster To store the data special directories have been made available to the users with name \u201cscratch and home\u201d the path to this directory is \u201c/gpfs-scratch\u201d and \u201c/gpfs-home\u201d. Whereas these directories are common to all the users, a user will get his own directory with their username in /gpfs-scratch/ as well as /home-home/ directories where they can store their data. However, there is limit to the storage provided to the users, the limits have been defined according to quota over these directories, all users will be allotted same quota by default. When a user wishes to transfer data from their local system (laptop/desktop) to HPC system, they can use various methods and tools A user using the \u2018Windows\u2019 operating system will get methods and tools that are native to Microsoft Windows and tools that could be installed on your Microsoft Windows machine.Linux operating system users do not require any tool. They can just use \u201cscp\u201d command on their terminal, as mentioned below. Users are advised to keep a copy of their data with themselves, once the project/research work is completed by transferring the data in from Madhava HPC Cluster to their local system (laptop/desktop). The command shown below can be used for effecting file transfers (In all the tools): scp \u2013r [path to the local data directory] [your username]@[IP of Madhava HPC:[path to directory on HPC where to save the data] for example: scp \u2013r /dir/dir/file sajil@:/gpfs-home/sajil Same Command could be used to transfer data from HPC system to your local system (laptop/desktop). scp \u2013r [your username]@[IP of Madhava HPC:[path todirectory on HPC where to save the data] [path to the local data directory] for example: scp \u2013r sajil@[cluster IP/Name]:/gpfs-home/sajil/file /dir/dir/file Running Interactive Jobs \u00b6 In general, the jobs can be run in an interactive manner or in batch mode. You can run an interactive job as follows: The following command asks for a single core in testq for one hour with a default amount of memory. srun --nodes=1 --ntasks-per-node=1 --time=01:00:00 \u2013partition=testq \u2013job-name= job-name --pty /usr/bin/bash The command prompt will appear as soon as the job starts. This is how it looks once the interactive job starts: image Exit the bash shell to end the job. If you exceed the time or memory limits the job will also abort. Please note that Madhava HPC Cluster is NOT meant for executing interactive jobs. However, for the purpose of quickly ascertaining successful run of a job before submitting a large job in batch (with large iteration counts), this can be used. This can even be used for running small jobs. The point to be kept in mind is that, since others too would be using this node, it is prudent not to inconvenience them by running large jobs. Managing Jobs \u00b6 Madhava HPC Cluster extensively uses modules. The purpose of the module is to provide the production environment for a given application, outside of the application itself. This also specifies which version of the application is available for a given session. All applications and libraries are made available through module files. A User has to load the appropriate module from the available modules. module avail # This command lists all the available modules module load compilers/intel/parallel_studio_xe_2020.4.912 # This will load the intel compilers into your environment module unload compilers/intel/parallel_studio_xe_2020.4.912 # This will remove all environment setting related to intel compiler loaded previously module list #This will list currently loaded modules. image A simple Slurm job script #!/bin/sh #SBATCH --nodes 1 // specifies number of nodes #SBATCH --ntasks-per-node=40 // specifies core per node #SBATCH --time=06:50:20 // specifies maximum duration of run #SBATCH --job-name=lammps // specifies job name #SBATCH --output=job_output.txt //specifies output file name #SBATCH --partition=shortq // specifies queue name #SBATCH --ntasks=10 //specifies total no of cores required ########################################################## echo \u201cJob Submitted\u201d #load required modules module load ... #------------- run your commands here\u2014-----------------# mpirun -n $SLURM_NTASKS ... echo \u201cJob finished successfully\u201d List Partition \u00b6 sinfo displays information about nodes and partitions(queues). sinfo Submit the Job \u00b6 To Submitting a simple standalone job , This is a simple submit script which is to be submitted sbatch job-script-name List jobs \u00b6 Monitoring jobs on SLURM can be done using the command squeue. squeue is used to view job and job step information for jobs managed by SLURM squeue Get job details \u00b6 scontrol can be used to report more detailed information about nodes, partitions, jobs, job steps, and configuration. scontrol show node - shows detailed information about compute nodes. scontrol show partition - shows detailed information about a specific partition scontrol show job - shows detailed information about a specific job or all jobs if no job id isgiven. scontrol update job - change attributes of submitted job. Suspend a job (root only): \u00b6 # scontrol suspend 135 # squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 135 shortq simple.s user1 S 0:13 1 compute01 Resume a job (root only): \u00b6 # scontrol resume 135 # squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 135 shortq simple.s user1 R 0:13 1 compute01 Kill a job. Users can kill their own jobs, root can kill any job. $ scancel 135 $ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) Hold a job: \u00b6 $ scontrol hold 139 Release a job: $ scontrol release 139 More about Batch Jobs (SLURM) \u00b6 SLURM (Simple Linux Utility for Resource Management) is a workload manager that provides a framework for job queues, allocation of compute nodes, and the start and execution of jobs. It is important to note: Compilations are done on the login node. Only the execution is scheduled via SLURM on the compute/GPU nodes Upon Submission of a Job script, each job gets a unique Job Id. This can be obtained from the \u2018squeue\u2019 command. The Job Id is also appended to the output and error filenames. Parameters used in SLURM job Script \u00b6 The job flags are used with SBATCH command. The syntax for the SLURM directive in a script is \"#SBATCH \". Some of the flags are used with the srun and salloc commands. Resource Flag Syntax Description partition --partition=partition-name Partition is a queue for jobs. time --time=01:00:00 Time limit for the job. nodes --nodes=2 Number of compute nodes for for the job cpus/cores --ntasks-per-node=8 Corresponds to number of cores on the compute node job name --job-name=\"job-1\" Name of job output file --output=job-1.out Name of file for stdout. access --exclusive Exclusive access to computenodes resource --gres=gpu:2 Request use of GPUs on compute node Some useful SLURM commands. \u00b6 Sl No Purpose Command 1 To check the queue status squeue 2 To check the status/ availability of nodes sinfo 3 To cancel a job running scancel note: job id can be obtained by command squeue 4 To check the jobs of a particular user squeue -u 5 To list all running jobs of a user squeue -u -t RUNNING 6 To list all pending jobs of a user squeue -u -t PENDING 7 To cancel all the pending jobs for a user scancel -t PENDING -u username Addressing Basic Security Concerns \u00b6 Your account on Madhava HPC Cluster is \u2018private to you\u2019. You are responsible for any actions emanating from your account. It is suggested that you should never share the password to anyone. Please note that, by default, a new account created on Madhava HPC Cluster is readable by everyone on the system. The following simple commands will make your account adequately safe. Command Discription chmod 700 [directory name] will ensure that only yourself can read, write and execute files in your directory chmod 750 [directory name] will enable yourself and the members of your group to read and execute files in your directory chmod 775 [directory name] will enable yourself, your group members and everyone else to read and execute files in your directory chmod 777 [directory name] will enable EVERY ONE on the system to read, write and execute files in your directory. This is a sort of \u2018free for all\u2019 situation. This should be used very judiciously","title":"Getting started"},{"location":"getting-started/#introduction","text":"Centre for Computational Modelling and Simulation (CCMS) aims to promote and support computational modelling as a mainstream research activity amongst the faculty and researchers of NIT Calicut. A High Performance Computing (HPC) Cluster and NVIDIA DGX Station purchased under HEFA are the major computing facility under this centre. The facility consists of 31 CPU nodes and 2 GPU nodes. The NVIDIA DGX Station has four V100 GPU accelerators. The facility is mainly for developing and running parallel codes for research purposes. The HPC system installed has been ranked as the 38 th in the list of top 100 computing machines in India, in January 2021, a list maintained by CDAC, Bangalore click here .","title":"Introduction"},{"location":"getting-started/#system-configuration","text":"Wanted to know about system configuration \ud83d\ude00? Check the it here","title":"System Configuration"},{"location":"getting-started/#get-an-account-for-you","text":"Centre for computational Modelling and Simulation (CCMS) comprises of a High Performance Computing cluster and a DGX Station that are available to both faculty and students. Inorder to get an account you may download the application and submit filled application to the mail id : ccmsadmin@nitc.ac.in or through hardcopy You will be notified once your account is approved. For NITC students, recommendation from the guide/ faculty is mandatory. A short proposal describing the computing to be carried out, justification for the use of HPC facility and expected outcome (1 page only) is to be attached with the submitted form.","title":"Get an Account for You"},{"location":"getting-started/#system-access","text":"The cluster can be accessed through Master node, which allows users to login, to submit jobs, transfer data and to compile source code. (If your compilation takes more than a few minutes, you should submit the compilation job into the queue to be run on the cluster.) By default, a user will have access to home directory (/gpfs-home/) This directory is available on the login node as well as the other nodes on the cluster. And the /gpfs-scratch/ directory may be used for temporary data storage, generally used to store data required for running jobs. Any data stored in /gpfs-scratch will be deleted after 30 days.","title":"System Access"},{"location":"getting-started/#remote-access","text":"","title":"Remote Access"},{"location":"getting-started/#using-ssh-in-windows","text":"Windows systems do not have any built-in support for using SSH, so you will have to download a software package to use SSH. PuTTY is the most popular open source (ie free) SSH client for Windows. To install it, visit the download site, and download the Installer package. Once installed, find the PuTTy application shortcut in your Start Menu, desktop. On clicking the PuTTy icon The PuTTy Configuration dialog should appear: Locate the Host Name input box in the PuTTy Configuration screen. Enter the server name you wish to connect to (e.g. [username]@[hpcipaddress]), and click Open. Enter your password when prompted, and press Enter. You are now connected!","title":"Using ssh in Windows"},{"location":"getting-started/#using-ssh-in-mac-or-linux","text":"Both Mac and Linux systems provide a built-in SSH client, so there is no need to install an additional package. Simply locate and run the Terminal app. Once in the terminal, you can connect to an SSH server by typing the following command: // to access from your pc user@my-pc:~$ ssh username@hostid // if a remote GUI section is required user@my-pc:~$ ssh username@hostid -X Note Your user name and the hostid will be recieved through mail. Please check that. and replace username & hostid with appropriate value How to change the user password? Use the \"passwd\" command to change the password for the user from the login node.","title":"Using ssh in Mac or Linux"},{"location":"getting-started/#usage-policy-and-guidelines","text":"Application software available in the system / installed by user is to be used for academic purpose only and cannot be used for the monetary benefit of an individual or a company. To protect the security of the system, the user should neither provide his/her password nor allow other individuals to use his/her account. The system administrators may verify the above fact at any point of time. If found guilty, the user id will be cancelled without further reference to the user. Individuals who attempt to use accounts, files, system resources or other facilities without authorization or those who aid other individuals doing so, may be committing a criminal act and may be subjected to criminal prosecution. It is the responsibility of each individual user to know what effects the use of certain programs and/or facilities can have on other users and/or facilities, whether it may damage system resources or severely inconvenience other users currently using the system. System files and other application software installed by users and provided under license are not to be copied or tampered with. A Project Report is to be submitted at the end of the project.(1 page max.) Acknowledgement of the use of the center\u2019s facilities should be made in journal publications, dissertations, theses, conference publications and reports published by the users. Any outcomes of the project, in terms of publications (journal / conference proceedings etc.) should be communicated to the center. Student Accounts will be deleted and the user files removed on graduation.","title":"Usage Policy and Guidelines"},{"location":"getting-started/#best-practices-of-hpc","text":"Do not go over your storage quota. Exceeding your storage quota can lead to many problems including batch jobs failing, confusing error messages and the inability to use X11 forwarding. Be sure to routinely run the \"du -sh ~/ \" command to check your usage. If more space is needed then remove files. Do not run jobs on the Master node. When you connect to a cluster via SSH you land on the Master node which is shared by all users. The login node is reserved for submitting jobs, compiling codes, installing software and running short tests that use only a few CPU-cores and finish within a few minutes. Anything more intensive must be submitted to the Slurm job scheduler as either a batch or interactive job. Failure to comply with this rule may result in your account being suspended. Do not try to access the Internet from batch or interactive jobs. All jobs submitted to the Slurm job scheduler run on the compute nodes which do not have Internet access. Because of this, a running job cannot download files, install packages or connect to GitHub. You will need to perform these operations on the login node before submitting the job. Do not allocate more than one CPU-core for serial jobs. Serial codes cannot run in parallel so using more than one CPU-core will not cause the job to run faster. Instead, doing so will waste resources. See the Slurm page for tips on figuring out if your code can run in parallel and for information about Job Arrays which allow one to run many jobs simultaneously. Do not run jobs with a parallel code without first conducting a scaling analysis. If your code runs in parallel then you need to determine the optimal number of nodes and CPU-cores to use. The same is true if it can use multiple GPUs. To do this, perform a scaling analysis as described in Choosing the Number of Nodes, CPU-cores and GPUs. Do not request a GPU for a code that can only use CPUs. Only codes that have been explicitly written to use GPUs can take advantage of GPUs. Allocating a GPU for a CPU-only code will not speed-up the execution time but it will increase your queue time, waste resources. Furthermore, some codes are written to use only a single GPU. For more see GPU Computing and Choosing the Number of Nodes, CPU-cores and GPUs. Do not load environment modules using only the partial name. You should always specify the full name of the environment module (e.g., module load compilers/intel/parallel_studio_xe_2020.4.912) and on some clusters failing to do so will result in an error. Also, you should avoid loading environment modules in your ~/.bashrc file. Instead, do this in Slurm scripts and on the command line when needed. Please do not use spaces while creating the directories and files.","title":"Best Practices of HPC"},{"location":"getting-started/#transferring-files-between-local-machine-and-hpc-cluster","text":"Users need to have the data and application related to their project/research work on Madhava HPC Cluster To store the data special directories have been made available to the users with name \u201cscratch and home\u201d the path to this directory is \u201c/gpfs-scratch\u201d and \u201c/gpfs-home\u201d. Whereas these directories are common to all the users, a user will get his own directory with their username in /gpfs-scratch/ as well as /home-home/ directories where they can store their data. However, there is limit to the storage provided to the users, the limits have been defined according to quota over these directories, all users will be allotted same quota by default. When a user wishes to transfer data from their local system (laptop/desktop) to HPC system, they can use various methods and tools A user using the \u2018Windows\u2019 operating system will get methods and tools that are native to Microsoft Windows and tools that could be installed on your Microsoft Windows machine.Linux operating system users do not require any tool. They can just use \u201cscp\u201d command on their terminal, as mentioned below. Users are advised to keep a copy of their data with themselves, once the project/research work is completed by transferring the data in from Madhava HPC Cluster to their local system (laptop/desktop). The command shown below can be used for effecting file transfers (In all the tools): scp \u2013r [path to the local data directory] [your username]@[IP of Madhava HPC:[path to directory on HPC where to save the data] for example: scp \u2013r /dir/dir/file sajil@:/gpfs-home/sajil Same Command could be used to transfer data from HPC system to your local system (laptop/desktop). scp \u2013r [your username]@[IP of Madhava HPC:[path todirectory on HPC where to save the data] [path to the local data directory] for example: scp \u2013r sajil@[cluster IP/Name]:/gpfs-home/sajil/file /dir/dir/file","title":"Transferring files between local machine and HPC cluster"},{"location":"getting-started/#running-interactive-jobs","text":"In general, the jobs can be run in an interactive manner or in batch mode. You can run an interactive job as follows: The following command asks for a single core in testq for one hour with a default amount of memory. srun --nodes=1 --ntasks-per-node=1 --time=01:00:00 \u2013partition=testq \u2013job-name= job-name --pty /usr/bin/bash The command prompt will appear as soon as the job starts. This is how it looks once the interactive job starts: image Exit the bash shell to end the job. If you exceed the time or memory limits the job will also abort. Please note that Madhava HPC Cluster is NOT meant for executing interactive jobs. However, for the purpose of quickly ascertaining successful run of a job before submitting a large job in batch (with large iteration counts), this can be used. This can even be used for running small jobs. The point to be kept in mind is that, since others too would be using this node, it is prudent not to inconvenience them by running large jobs.","title":"Running Interactive Jobs"},{"location":"getting-started/#managing-jobs","text":"Madhava HPC Cluster extensively uses modules. The purpose of the module is to provide the production environment for a given application, outside of the application itself. This also specifies which version of the application is available for a given session. All applications and libraries are made available through module files. A User has to load the appropriate module from the available modules. module avail # This command lists all the available modules module load compilers/intel/parallel_studio_xe_2020.4.912 # This will load the intel compilers into your environment module unload compilers/intel/parallel_studio_xe_2020.4.912 # This will remove all environment setting related to intel compiler loaded previously module list #This will list currently loaded modules. image A simple Slurm job script #!/bin/sh #SBATCH --nodes 1 // specifies number of nodes #SBATCH --ntasks-per-node=40 // specifies core per node #SBATCH --time=06:50:20 // specifies maximum duration of run #SBATCH --job-name=lammps // specifies job name #SBATCH --output=job_output.txt //specifies output file name #SBATCH --partition=shortq // specifies queue name #SBATCH --ntasks=10 //specifies total no of cores required ########################################################## echo \u201cJob Submitted\u201d #load required modules module load ... #------------- run your commands here\u2014-----------------# mpirun -n $SLURM_NTASKS ... echo \u201cJob finished successfully\u201d","title":"Managing Jobs"},{"location":"getting-started/#list-partition","text":"sinfo displays information about nodes and partitions(queues). sinfo","title":"List Partition"},{"location":"getting-started/#submit-the-job","text":"To Submitting a simple standalone job , This is a simple submit script which is to be submitted sbatch job-script-name","title":"Submit the Job"},{"location":"getting-started/#list-jobs","text":"Monitoring jobs on SLURM can be done using the command squeue. squeue is used to view job and job step information for jobs managed by SLURM squeue","title":"List jobs"},{"location":"getting-started/#get-job-details","text":"scontrol can be used to report more detailed information about nodes, partitions, jobs, job steps, and configuration. scontrol show node - shows detailed information about compute nodes. scontrol show partition - shows detailed information about a specific partition scontrol show job - shows detailed information about a specific job or all jobs if no job id isgiven. scontrol update job - change attributes of submitted job.","title":"Get job details"},{"location":"getting-started/#suspend-a-job-root-only","text":"# scontrol suspend 135 # squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 135 shortq simple.s user1 S 0:13 1 compute01","title":"Suspend a job (root only):"},{"location":"getting-started/#resume-a-job-root-only","text":"# scontrol resume 135 # squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 135 shortq simple.s user1 R 0:13 1 compute01 Kill a job. Users can kill their own jobs, root can kill any job. $ scancel 135 $ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)","title":"Resume a job (root only):"},{"location":"getting-started/#hold-a-job","text":"$ scontrol hold 139 Release a job: $ scontrol release 139","title":"Hold a job:"},{"location":"getting-started/#more-about-batch-jobs-slurm","text":"SLURM (Simple Linux Utility for Resource Management) is a workload manager that provides a framework for job queues, allocation of compute nodes, and the start and execution of jobs. It is important to note: Compilations are done on the login node. Only the execution is scheduled via SLURM on the compute/GPU nodes Upon Submission of a Job script, each job gets a unique Job Id. This can be obtained from the \u2018squeue\u2019 command. The Job Id is also appended to the output and error filenames.","title":"More about Batch Jobs (SLURM)"},{"location":"getting-started/#parameters-used-in-slurm-job-script","text":"The job flags are used with SBATCH command. The syntax for the SLURM directive in a script is \"#SBATCH \". Some of the flags are used with the srun and salloc commands. Resource Flag Syntax Description partition --partition=partition-name Partition is a queue for jobs. time --time=01:00:00 Time limit for the job. nodes --nodes=2 Number of compute nodes for for the job cpus/cores --ntasks-per-node=8 Corresponds to number of cores on the compute node job name --job-name=\"job-1\" Name of job output file --output=job-1.out Name of file for stdout. access --exclusive Exclusive access to computenodes resource --gres=gpu:2 Request use of GPUs on compute node","title":"Parameters used in SLURM job Script"},{"location":"getting-started/#some-useful-slurm-commands","text":"Sl No Purpose Command 1 To check the queue status squeue 2 To check the status/ availability of nodes sinfo 3 To cancel a job running scancel note: job id can be obtained by command squeue 4 To check the jobs of a particular user squeue -u 5 To list all running jobs of a user squeue -u -t RUNNING 6 To list all pending jobs of a user squeue -u -t PENDING 7 To cancel all the pending jobs for a user scancel -t PENDING -u username","title":"Some useful SLURM commands."},{"location":"getting-started/#addressing-basic-security-concerns","text":"Your account on Madhava HPC Cluster is \u2018private to you\u2019. You are responsible for any actions emanating from your account. It is suggested that you should never share the password to anyone. Please note that, by default, a new account created on Madhava HPC Cluster is readable by everyone on the system. The following simple commands will make your account adequately safe. Command Discription chmod 700 [directory name] will ensure that only yourself can read, write and execute files in your directory chmod 750 [directory name] will enable yourself and the members of your group to read and execute files in your directory chmod 775 [directory name] will enable yourself, your group members and everyone else to read and execute files in your directory chmod 777 [directory name] will enable EVERY ONE on the system to read, write and execute files in your directory. This is a sort of \u2018free for all\u2019 situation. This should be used very judiciously","title":"Addressing Basic Security Concerns"},{"location":"introduction/","text":"Introduction \u00b6 MM/PB(GB)SA method can be used for calculating binding free energies of non covalently bound complexes. Figure 1. Thermodynamic cycle for binding free energy calculations The free binding energy for a complex can be estimated as follows: \u2206\ud835\udc3a \ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc51 = \u2329\ud835\udc3a \ud835\udc36\ud835\udc42\ud835\udc40 \u232a\u2212\u2329\ud835\udc3a \ud835\udc45\ud835\udc38\ud835\udc36 \u232a\u2212\u2329\ud835\udc3a \ud835\udc3f\ud835\udc3c\ud835\udc3a \u232a (1) where each term to the right in the equation is given by: \u2329\ud835\udc3a \ud835\udc65 \u232a = \u2329\ud835\udc38 \ud835\udc40\ud835\udc40 \u232a + \u2329\ud835\udc3a \ud835\udc60\ud835\udc5c\ud835\udc59 \u232a \u2212 \u2329\ud835\udc47\ud835\udc46\u232a (2) In turn, \u2206\ud835\udc3a \ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc51 can also be represented as: \u2206\ud835\udc3a \ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc51 = \u2206\ud835\udc3b \u2212 \ud835\udc47\u2206\ud835\udc46 (3) where \u2206\ud835\udc3b corresponds to the enthalpy of binding and \u2212\ud835\udc47\u2206\ud835\udc46 to the conformational entropy after ligand binding. When the entropic term is dismissed, the computed value is the effective free energy, which is usually sufficient for comparing relative binding free energies of related ligands. The \u2206\ud835\udc3b can be decomposed into different terms: \u2206\ud835\udc3b = \u2206\ud835\udc38 \ud835\udc40\ud835\udc40 + \u2206\ud835\udc3a \ud835\udc60\ud835\udc5c\ud835\udc59 (4) where: \u2206\ud835\udc38 \ud835\udc40\ud835\udc40 = \u2206\ud835\udc38 \ud835\udc4f\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc51 + \u2206\ud835\udc38 \ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc4f\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc51 = (\u2206\ud835\udc38 \ud835\udc4f\ud835\udc5c\ud835\udc5b\ud835\udc51 + \u2206\ud835\udc38 \ud835\udc4e\ud835\udc5b\ud835\udc54\ud835\udc59\ud835\udc52 + \u2206\ud835\udc38 \ud835\udc51\ud835\udc56\u210e\ud835\udc52\ud835\udc51\ud835\udc5f\ud835\udc4e\ud835\udc59 ) + (\u2206\ud835\udc38 \ud835\udc52\ud835\udc59\ud835\udc52 + \u2206\ud835\udc38 \ud835\udc63\ud835\udc51\ud835\udc4a ) (5) The gas phase free energy contributions (\u2206\ud835\udc38 \ud835\udc40\ud835\udc40 ) are calculated by sander within the AmberTools package according to the force field used in the MD simulation. The \u2206\ud835\udc3a \ud835\udc60\ud835\udc5c\ud835\udc59 is given by: \u2206\ud835\udc3a \ud835\udc60\ud835\udc5c\ud835\udc59 = \u2206\ud835\udc3a \ud835\udc5d\ud835\udc5c\ud835\udc59 + \u2206\ud835\udc3a \ud835\udc5b\ud835\udc5c\ud835\udc5b\u2212\ud835\udc5d\ud835\udc5c\ud835\udc59 = \u2206\ud835\udc3a \ud835\udc43\ud835\udc35/\ud835\udc3a\ud835\udc35 + \u2206\ud835\udc3a \ud835\udc5b\ud835\udc5c\ud835\udc5b\u2212\ud835\udc5d\ud835\udc5c\ud835\udc59 (6) where: \u2206\ud835\udc3a \ud835\udc5b\ud835\udc5c\ud835\udc5b\u2212\ud835\udc5d\ud835\udc5c\ud835\udc59\ud835\udc4e\ud835\udc5f = \ud835\udc41\ud835\udc43 \ud835\udc47\ud835\udc38\ud835\udc41\ud835\udc46\ud835\udc3c\ud835\udc42\ud835\udc41 \u2217 \u2206\ud835\udc46\ud835\udc34\ud835\udc46\ud835\udc34 + \ud835\udc41\ud835\udc43 \ud835\udc42\ud835\udc39\ud835\udc39\ud835\udc46\ud835\udc38\ud835\udc47 (7) or, \u2206\ud835\udc3a \ud835\udc5b\ud835\udc5c\ud835\udc5b\u2212\ud835\udc5d\ud835\udc5c\ud835\udc59 = \u2206\ud835\udc3a \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc5d + \u2206\ud835\udc3a \ud835\udc50\ud835\udc4e\ud835\udc63\ud835\udc56\ud835\udc61\ud835\udc66 = \u2206\ud835\udc3a \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc5d + (\ud835\udc36\ud835\udc34\ud835\udc49\ud835\udc3c\ud835\udc47\ud835\udc4c \ud835\udc47\ud835\udc38\ud835\udc41\ud835\udc46\ud835\udc3c\ud835\udc42\ud835\udc41 \u2217 \u2206\ud835\udc46\ud835\udc34\ud835\udc46\ud835\udc34 + \ud835\udc36\ud835\udc34\ud835\udc49\ud835\udc3c\ud835\udc47\ud835\udc4c \ud835\udc42\ud835\udc39\ud835\udc39\ud835\udc46\ud835\udc38\ud835\udc47 ) (8) In the above equations, \u2206\ud835\udc38 \ud835\udc40\ud835\udc40 corresponds to the molecular mechanical energy changes in the gas phase. \u2206\ud835\udc38 \ud835\udc40\ud835\udc40 includes \u2206\ud835\udc38 \ud835\udc4f\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc51 , also known as internal energy, and \u2206\ud835\udc38 \ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc4f\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc51 , corresponding to the van der Waals and electrostatic contributions. The solvation energy is determined differently, depending on the method employed. In the 3D-RISM model, both components -polar and non-polar- of the solvation energy are calculated. However, the PB and GB models estimate only the polar component of the solvation. The non-polar component is usually assumed to be proportional to the molecule's total solvent accessible surface area (SASA), with a proportionality constant derived from experimental solvation energies of small non-polar molecules (eq. 7). Alternatively, a modern approach that separates non-polar solvation free energies into cavity and dispersion terms can be used. In this approach, SASA is used to correlate the cavity term only, while a surface-integration method is employed to compute the dispersion term (eq. 8). Furthermore, the entropic component is usually calculated by normal modes analysis (NMODE). The translational and rotational entropies can be estimated using standard statistical mechanical formulas. Nevertheless, calculating vibrational entropy using normal modes is computationally expensive because it requires expanding the internal coordinate covariance matrix for all degrees of freedom for a set of minimized structures. Conversely, the Quasi-harmonic (QH) approximation is less computationally expensive, although it requires a considerable number of frames to converge. Recently, other alternatives have been developed, such as NMODE in truncated systems, which considerably reduces the computational cost. Interaction Entropy (IE) is another novel method that calculates the entropic component of the binding free energy directly from MD simulations without any extra computational cost. This method is numerically reliable, more computationally efficient, and superior to the standard NMODE approach, as shown in an extensive study of over a dozen randomly selected protein-ligand binding systems. Typically, two approaches are used for MM/PB(GB)SA calculations, known as Single Trajectory Protocol (STP) and Multiple Trajectory Protocol (MTP). In STP, both the receptor and the ligand trajectories are extracted from that of the complex. This approach is valid when the bound and unbound states of the receptor, and the ligand are similar. It is computationally less expensive than the MTP approach since only a simulation of the complex is required. Additionally, the potential internal terms ( e.g. , bonds, angles, and dihedrals) cancel exactly in STP since these terms are the same in both bound and unbound states. On the other hand, the MTP is a more realistic approach because it considers multiple trajectories ( i.e. , complex, receptor, and ligand). However, significant conformational changes can lead to numerous errors. In practice, a detailed study of the system is required to select the approach to be used. Literature \u00b6 Further information can be found in Amber manual : MMPBSA.py The Generalized Born/Surface Area Model PBSA Reference Interaction Site Model Generalized Born (GB) for QM/MM calculations and the foundational papers: Srinivasan J. et al., 1998 Kollman P. A. et al., 2000 Gohlke H., Case D. A. 2004 as well as some reviews and expert opinions: Genheden S., Ryde U. 2015 Wang et. al., 2018 Wang et. al., 2019 Tuccinardi, 2021","title":"Introduction"},{"location":"introduction/#introduction","text":"MM/PB(GB)SA method can be used for calculating binding free energies of non covalently bound complexes. Figure 1. Thermodynamic cycle for binding free energy calculations The free binding energy for a complex can be estimated as follows: \u2206\ud835\udc3a \ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc51 = \u2329\ud835\udc3a \ud835\udc36\ud835\udc42\ud835\udc40 \u232a\u2212\u2329\ud835\udc3a \ud835\udc45\ud835\udc38\ud835\udc36 \u232a\u2212\u2329\ud835\udc3a \ud835\udc3f\ud835\udc3c\ud835\udc3a \u232a (1) where each term to the right in the equation is given by: \u2329\ud835\udc3a \ud835\udc65 \u232a = \u2329\ud835\udc38 \ud835\udc40\ud835\udc40 \u232a + \u2329\ud835\udc3a \ud835\udc60\ud835\udc5c\ud835\udc59 \u232a \u2212 \u2329\ud835\udc47\ud835\udc46\u232a (2) In turn, \u2206\ud835\udc3a \ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc51 can also be represented as: \u2206\ud835\udc3a \ud835\udc4f\ud835\udc56\ud835\udc5b\ud835\udc51 = \u2206\ud835\udc3b \u2212 \ud835\udc47\u2206\ud835\udc46 (3) where \u2206\ud835\udc3b corresponds to the enthalpy of binding and \u2212\ud835\udc47\u2206\ud835\udc46 to the conformational entropy after ligand binding. When the entropic term is dismissed, the computed value is the effective free energy, which is usually sufficient for comparing relative binding free energies of related ligands. The \u2206\ud835\udc3b can be decomposed into different terms: \u2206\ud835\udc3b = \u2206\ud835\udc38 \ud835\udc40\ud835\udc40 + \u2206\ud835\udc3a \ud835\udc60\ud835\udc5c\ud835\udc59 (4) where: \u2206\ud835\udc38 \ud835\udc40\ud835\udc40 = \u2206\ud835\udc38 \ud835\udc4f\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc51 + \u2206\ud835\udc38 \ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc4f\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc51 = (\u2206\ud835\udc38 \ud835\udc4f\ud835\udc5c\ud835\udc5b\ud835\udc51 + \u2206\ud835\udc38 \ud835\udc4e\ud835\udc5b\ud835\udc54\ud835\udc59\ud835\udc52 + \u2206\ud835\udc38 \ud835\udc51\ud835\udc56\u210e\ud835\udc52\ud835\udc51\ud835\udc5f\ud835\udc4e\ud835\udc59 ) + (\u2206\ud835\udc38 \ud835\udc52\ud835\udc59\ud835\udc52 + \u2206\ud835\udc38 \ud835\udc63\ud835\udc51\ud835\udc4a ) (5) The gas phase free energy contributions (\u2206\ud835\udc38 \ud835\udc40\ud835\udc40 ) are calculated by sander within the AmberTools package according to the force field used in the MD simulation. The \u2206\ud835\udc3a \ud835\udc60\ud835\udc5c\ud835\udc59 is given by: \u2206\ud835\udc3a \ud835\udc60\ud835\udc5c\ud835\udc59 = \u2206\ud835\udc3a \ud835\udc5d\ud835\udc5c\ud835\udc59 + \u2206\ud835\udc3a \ud835\udc5b\ud835\udc5c\ud835\udc5b\u2212\ud835\udc5d\ud835\udc5c\ud835\udc59 = \u2206\ud835\udc3a \ud835\udc43\ud835\udc35/\ud835\udc3a\ud835\udc35 + \u2206\ud835\udc3a \ud835\udc5b\ud835\udc5c\ud835\udc5b\u2212\ud835\udc5d\ud835\udc5c\ud835\udc59 (6) where: \u2206\ud835\udc3a \ud835\udc5b\ud835\udc5c\ud835\udc5b\u2212\ud835\udc5d\ud835\udc5c\ud835\udc59\ud835\udc4e\ud835\udc5f = \ud835\udc41\ud835\udc43 \ud835\udc47\ud835\udc38\ud835\udc41\ud835\udc46\ud835\udc3c\ud835\udc42\ud835\udc41 \u2217 \u2206\ud835\udc46\ud835\udc34\ud835\udc46\ud835\udc34 + \ud835\udc41\ud835\udc43 \ud835\udc42\ud835\udc39\ud835\udc39\ud835\udc46\ud835\udc38\ud835\udc47 (7) or, \u2206\ud835\udc3a \ud835\udc5b\ud835\udc5c\ud835\udc5b\u2212\ud835\udc5d\ud835\udc5c\ud835\udc59 = \u2206\ud835\udc3a \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc5d + \u2206\ud835\udc3a \ud835\udc50\ud835\udc4e\ud835\udc63\ud835\udc56\ud835\udc61\ud835\udc66 = \u2206\ud835\udc3a \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc5d + (\ud835\udc36\ud835\udc34\ud835\udc49\ud835\udc3c\ud835\udc47\ud835\udc4c \ud835\udc47\ud835\udc38\ud835\udc41\ud835\udc46\ud835\udc3c\ud835\udc42\ud835\udc41 \u2217 \u2206\ud835\udc46\ud835\udc34\ud835\udc46\ud835\udc34 + \ud835\udc36\ud835\udc34\ud835\udc49\ud835\udc3c\ud835\udc47\ud835\udc4c \ud835\udc42\ud835\udc39\ud835\udc39\ud835\udc46\ud835\udc38\ud835\udc47 ) (8) In the above equations, \u2206\ud835\udc38 \ud835\udc40\ud835\udc40 corresponds to the molecular mechanical energy changes in the gas phase. \u2206\ud835\udc38 \ud835\udc40\ud835\udc40 includes \u2206\ud835\udc38 \ud835\udc4f\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc51 , also known as internal energy, and \u2206\ud835\udc38 \ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc4f\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc51 , corresponding to the van der Waals and electrostatic contributions. The solvation energy is determined differently, depending on the method employed. In the 3D-RISM model, both components -polar and non-polar- of the solvation energy are calculated. However, the PB and GB models estimate only the polar component of the solvation. The non-polar component is usually assumed to be proportional to the molecule's total solvent accessible surface area (SASA), with a proportionality constant derived from experimental solvation energies of small non-polar molecules (eq. 7). Alternatively, a modern approach that separates non-polar solvation free energies into cavity and dispersion terms can be used. In this approach, SASA is used to correlate the cavity term only, while a surface-integration method is employed to compute the dispersion term (eq. 8). Furthermore, the entropic component is usually calculated by normal modes analysis (NMODE). The translational and rotational entropies can be estimated using standard statistical mechanical formulas. Nevertheless, calculating vibrational entropy using normal modes is computationally expensive because it requires expanding the internal coordinate covariance matrix for all degrees of freedom for a set of minimized structures. Conversely, the Quasi-harmonic (QH) approximation is less computationally expensive, although it requires a considerable number of frames to converge. Recently, other alternatives have been developed, such as NMODE in truncated systems, which considerably reduces the computational cost. Interaction Entropy (IE) is another novel method that calculates the entropic component of the binding free energy directly from MD simulations without any extra computational cost. This method is numerically reliable, more computationally efficient, and superior to the standard NMODE approach, as shown in an extensive study of over a dozen randomly selected protein-ligand binding systems. Typically, two approaches are used for MM/PB(GB)SA calculations, known as Single Trajectory Protocol (STP) and Multiple Trajectory Protocol (MTP). In STP, both the receptor and the ligand trajectories are extracted from that of the complex. This approach is valid when the bound and unbound states of the receptor, and the ligand are similar. It is computationally less expensive than the MTP approach since only a simulation of the complex is required. Additionally, the potential internal terms ( e.g. , bonds, angles, and dihedrals) cancel exactly in STP since these terms are the same in both bound and unbound states. On the other hand, the MTP is a more realistic approach because it considers multiple trajectories ( i.e. , complex, receptor, and ligand). However, significant conformational changes can lead to numerous errors. In practice, a detailed study of the system is required to select the approach to be used.","title":"Introduction"},{"location":"introduction/#literature","text":"Further information can be found in Amber manual : MMPBSA.py The Generalized Born/Surface Area Model PBSA Reference Interaction Site Model Generalized Born (GB) for QM/MM calculations and the foundational papers: Srinivasan J. et al., 1998 Kollman P. A. et al., 2000 Gohlke H., Case D. A. 2004 as well as some reviews and expert opinions: Genheden S., Ryde U. 2015 Wang et. al., 2018 Wang et. al., 2019 Tuccinardi, 2021","title":"Literature"},{"location":"queueing/","text":"","title":"Queuing System"},{"location":"softwarepackages/","text":"Scientific Software Packages \u00b6 This page provides links to the pages describing a number of the individual software packages installed centrally on ARCHER. The pages contain information on running jobs (including example job submission scripts). Build and Run Instructions GitHub Repository New compile and run instructions are now being added to a public GitHub repository at: https://github.com/HPC-UK/build-instructions and we are migrating all current compile instructions to this repository. Links in the individual pages below will be updated to point to the new instructions and notes as they are added. Contributing to the GitHub repository We are happy to accept contributions to the repository of build and run instructions ia pull requests. To contribute to this repository, first you have to fork it on GitHub and clone it to your machine, see Fork a Repo for the GitHub documentation on this process. Once you have made your changes and updated your Fork on GitHub you will need to Open a Pull Request. Amber/PMEMD A package of molecular simulation programs and analysis tools. abinit ABINIT is a package whose main program allows one to find the total energy, charge density and electronic structure of systems made of electrons and nuclei (molecules and periodic solids) within Density Functional Theory (DFT), using pseudopotentials and a planewave or wavelet basis. ABINIT also includes options to optimize the geometry according to the DFT forces and stresses, or to perform molecular dynamics simulations using these forces, or to generate dynamical matrices, Born effective charges, and dielectric tensors, based on Density-Functional Perturbation Theory, and many more properties. CASTEP A software package which uses density functional theory to provide a good atomic-level description of all manner of materials and molecules. CESM Community Earth System Model, or CESM, is a fully-coupled, community, global climate model that provides state-of-the-art computer simulations of the Earth's past, present, and future climate states. ChemShell A Tcl-based Chemistry code focusing on hybrid QM/MM calculations with support for standard quantum chemical or force field calculations. Code_Saturne Code_Saturne solves the Navier-Stokes equations for 2D, 2D-axisymmetric and 3D flows, steady or unsteady, laminar or turbulent, incompressible or weakly dilatable, isothermal or not, with scalars transport if required. Several turbulence models are available, from Reynolds-Averaged models to Large-Eddy Simulation models. In addition, a number of specific physical models are also available as \"modules\": gas, coal and heavy-fuel oil combustion, semi-transparent radiative transfer, particle-tracking with Lagrangian modeling, Joule effect, electrics arcs, weakly compressible flows, atmospheric flows, rotor/stator interaction for hydraulic machines. COSA COSA is a Computational Fluid Dynamics (CFD) suite of solvers of the Euler, laminar and turbulent compressible Navier-Stokes (NS) equations. All solvers are based on a finite volume discretisation of the physical domain by means of structured multi-block grids. CP2K A freely available program to perform atomistic and molecular simulations of solid state, liquid, molecular and biological systems. It provides a general framework for different methods such as e.g. density functional theory (DFT) using a mixed Gaussian and plane waves approach (GPW), and classical pair and many-body potentials. CPMD A parallelized plane wave/pseudopotential implementation of Density Functional Theory, particularly designed for ab-initio molecular dynamics. CRYSTAL CRYSTAL is a general-purpose program for the study of crystalline solids. The CRYSTAL program computes the electronic structure of periodic systems within Hartree Fock, density functional or various hybrid approximations (global, range-separated and double-hybrids). The Bloch functions of the periodic systems are expanded as linear combinations of atom centred Gaussian functions. Powerful screening techniques are used to exploit real space locality. Restricted (Closed Shell) and Unrestricted (Spin-polarized) calculations can be performed with all-electron and valence-only basis sets with effective core pseudo-potentials. Desmond A software package developed to perform high-speed molecular dynamics simulations of biological systems. DL_MESO A mesoscale simulation package providing dissipative particle dynamics and an implementation of the lattice Boltzmann equation (LBE) for complex fluids. DL_POLY A general purpose molecular dynamics simulation package. ELk An all-electron full-potential linearised augmented-plane wave (FP-LAPW) code with many advanced features. Written originally at Karl-Franzens-Universitt Graz as a milestone of the EXCITING EU Research and Training Network. Exciting An all-electron full-potential computer package providing the variety of different related basis sets, subsumed as (linearized) augmented planewave + local orbital (L)APW+lo methods. Fire Dynamics Simulator A computational fluid dynamics model of fire-driven fluid flow. FEniCS A software collection for the automated solution of differential equations. GAMESS GAMESS is a program for ab initio molecular quantum chemistry. Briefly, GAMESS can compute SCF wavefunctions ranging from RHF, ROHF, UHF, GVB, and MCSCF. Correlation corrections to these SCF wavefunctions include Configuration Interaction, second order perturbation Theory, and Coupled-Cluster approaches, as well as the Density Functional Theory approximation. GAMESS-UK GAMESS-UK is the general purpose ab initio molecular electronic structure program for performing SCF-, DFT- and MCSCF-gradient calculations, together with a variety of techniques for post Hartree Fock calculations GPAW GPAW is a density-functional theory (DFT) Python code based on the projector-augmented wave (PAW) method and the atomic simulation environment (ASE). GROMACS GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers. GS2 A package to study gyro-kinetic turbulence in fusion experiments and astrophysical plasmas. LAMMPS (Large-scale Atomic/Molecular Massively Parallel Simulator) a classical molecular dynamics code. Molpro Molpro is a complete system of ab initio programs for molecular electronic structure calculations, designed and maintained by H.-J. Werner and P. J. Knowles, and containing contributions from a number of other authors. As distinct from other commonly used quantum chemistry packages, the emphasis is on highly accurate computations, with extensive treatment of the electron correlation problem through the multiconfiguration-reference CI, coupled cluster and associated methods. NAG Library The NAG Library provides a collection of numerical and statistical algorithms which can be embedded into your codes on ARCHER at compile time. NAMD A parallel molecular dynamics application designed to simulate large bio-molecular systems. It is developed and maintained by the University of Illinois at Urbana-Champaign. NCIPLOT NCIPLOT is a program that enables the computation and graphical visualisation of inter- and intra-molecular non-covalent interactions (hydrogen bonds, ?-? interactions, ...) in systems ranging from small dimers to large biomolecules. NWChem NWChem aims to provide its users with computational chemistry tools that are scalable both in their ability to treat large scientific computational chemistry problems efficiently, and in their use of available parallel computing resources from high-performance parallel supercomputers to conventional workstation clusters. The NWChem software can handle: Biomolecules, nanostructures, and solid-state; From quantum to classical, and all combinations; Gaussian basis functions or plane-waves; Scaling from one to thousands of processors; Properties and relativity. ONETEP ONETEP (Order-N Electronic Total Energy Package) is a linear-scaling code for quantum-mechanical calculations based on density-functional theory OpenFOAM OpenFOAM is an open-source toolbox for computational fluid dynamics. OpenFOAM consists of generic tools to simulate complex physics for a variety of fields of interest, from fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics, electromagnetism and the pricing of financial options. The core technology of OpenFOAM is a flexible set of modules written in C++. These are used to build solvers and utilities to perform pre- and post-processing tasks ranging from simple data manipulation to visualisation and mesh processing. Paraview Paraview is a data visualisation and analysis package. Whilst ARCHER compute or login nodes do not have graphics cards installed in them paraview is installed so the visualisation libraries and applications can be used to post-process simulation data. To this end the pvserver application has been installed, along with the paraview libraries and client application. PLUMED PLUMED is a plugin that works with a large number of molecular dynamics codes. It can be used to analyse features of the dynamics on-the-fly or to perform a wide variety of free energy methods. Quantum Espresso Quantum Espresso is an integrated suite of Open-Source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials. SIESTA (Spanish Initiative for Electronic Simulations with Thousands of Atoms) is a program to perform electronic structure calculations and ab initio molecular dynamics simulations of molecules and solids. SLEPc (Scalable Library for Eigenvalue Problem Computations) a library for parallel computation of eigenvalues and eigenvectors. Based on PETSc. Note: not centrally installed as a module, only build instructions provided. TINKER The TINKER molecular modeling software is a complete and general package for molecular mechanics and dynamics, with some special features for biopolymers. VASP A package for ab initio, quantum-mechanical, molecular dynamics simulations. WIEN2k WIEN2k allows to perform electronic structure calculations of solids using density functional theory. It is based on the full-potential (linearized) augmented plane-wave and local orbitals method, one among the most accurate schemes for band structure calculations. WRF WRF is the Weather Research and Forecasting model.","title":"Madhava Software Packages"},{"location":"softwarepackages/#scientific-software-packages","text":"This page provides links to the pages describing a number of the individual software packages installed centrally on ARCHER. The pages contain information on running jobs (including example job submission scripts). Build and Run Instructions GitHub Repository New compile and run instructions are now being added to a public GitHub repository at: https://github.com/HPC-UK/build-instructions and we are migrating all current compile instructions to this repository. Links in the individual pages below will be updated to point to the new instructions and notes as they are added. Contributing to the GitHub repository We are happy to accept contributions to the repository of build and run instructions ia pull requests. To contribute to this repository, first you have to fork it on GitHub and clone it to your machine, see Fork a Repo for the GitHub documentation on this process. Once you have made your changes and updated your Fork on GitHub you will need to Open a Pull Request. Amber/PMEMD A package of molecular simulation programs and analysis tools. abinit ABINIT is a package whose main program allows one to find the total energy, charge density and electronic structure of systems made of electrons and nuclei (molecules and periodic solids) within Density Functional Theory (DFT), using pseudopotentials and a planewave or wavelet basis. ABINIT also includes options to optimize the geometry according to the DFT forces and stresses, or to perform molecular dynamics simulations using these forces, or to generate dynamical matrices, Born effective charges, and dielectric tensors, based on Density-Functional Perturbation Theory, and many more properties. CASTEP A software package which uses density functional theory to provide a good atomic-level description of all manner of materials and molecules. CESM Community Earth System Model, or CESM, is a fully-coupled, community, global climate model that provides state-of-the-art computer simulations of the Earth's past, present, and future climate states. ChemShell A Tcl-based Chemistry code focusing on hybrid QM/MM calculations with support for standard quantum chemical or force field calculations. Code_Saturne Code_Saturne solves the Navier-Stokes equations for 2D, 2D-axisymmetric and 3D flows, steady or unsteady, laminar or turbulent, incompressible or weakly dilatable, isothermal or not, with scalars transport if required. Several turbulence models are available, from Reynolds-Averaged models to Large-Eddy Simulation models. In addition, a number of specific physical models are also available as \"modules\": gas, coal and heavy-fuel oil combustion, semi-transparent radiative transfer, particle-tracking with Lagrangian modeling, Joule effect, electrics arcs, weakly compressible flows, atmospheric flows, rotor/stator interaction for hydraulic machines. COSA COSA is a Computational Fluid Dynamics (CFD) suite of solvers of the Euler, laminar and turbulent compressible Navier-Stokes (NS) equations. All solvers are based on a finite volume discretisation of the physical domain by means of structured multi-block grids. CP2K A freely available program to perform atomistic and molecular simulations of solid state, liquid, molecular and biological systems. It provides a general framework for different methods such as e.g. density functional theory (DFT) using a mixed Gaussian and plane waves approach (GPW), and classical pair and many-body potentials. CPMD A parallelized plane wave/pseudopotential implementation of Density Functional Theory, particularly designed for ab-initio molecular dynamics. CRYSTAL CRYSTAL is a general-purpose program for the study of crystalline solids. The CRYSTAL program computes the electronic structure of periodic systems within Hartree Fock, density functional or various hybrid approximations (global, range-separated and double-hybrids). The Bloch functions of the periodic systems are expanded as linear combinations of atom centred Gaussian functions. Powerful screening techniques are used to exploit real space locality. Restricted (Closed Shell) and Unrestricted (Spin-polarized) calculations can be performed with all-electron and valence-only basis sets with effective core pseudo-potentials. Desmond A software package developed to perform high-speed molecular dynamics simulations of biological systems. DL_MESO A mesoscale simulation package providing dissipative particle dynamics and an implementation of the lattice Boltzmann equation (LBE) for complex fluids. DL_POLY A general purpose molecular dynamics simulation package. ELk An all-electron full-potential linearised augmented-plane wave (FP-LAPW) code with many advanced features. Written originally at Karl-Franzens-Universitt Graz as a milestone of the EXCITING EU Research and Training Network. Exciting An all-electron full-potential computer package providing the variety of different related basis sets, subsumed as (linearized) augmented planewave + local orbital (L)APW+lo methods. Fire Dynamics Simulator A computational fluid dynamics model of fire-driven fluid flow. FEniCS A software collection for the automated solution of differential equations. GAMESS GAMESS is a program for ab initio molecular quantum chemistry. Briefly, GAMESS can compute SCF wavefunctions ranging from RHF, ROHF, UHF, GVB, and MCSCF. Correlation corrections to these SCF wavefunctions include Configuration Interaction, second order perturbation Theory, and Coupled-Cluster approaches, as well as the Density Functional Theory approximation. GAMESS-UK GAMESS-UK is the general purpose ab initio molecular electronic structure program for performing SCF-, DFT- and MCSCF-gradient calculations, together with a variety of techniques for post Hartree Fock calculations GPAW GPAW is a density-functional theory (DFT) Python code based on the projector-augmented wave (PAW) method and the atomic simulation environment (ASE). GROMACS GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers. GS2 A package to study gyro-kinetic turbulence in fusion experiments and astrophysical plasmas. LAMMPS (Large-scale Atomic/Molecular Massively Parallel Simulator) a classical molecular dynamics code. Molpro Molpro is a complete system of ab initio programs for molecular electronic structure calculations, designed and maintained by H.-J. Werner and P. J. Knowles, and containing contributions from a number of other authors. As distinct from other commonly used quantum chemistry packages, the emphasis is on highly accurate computations, with extensive treatment of the electron correlation problem through the multiconfiguration-reference CI, coupled cluster and associated methods. NAG Library The NAG Library provides a collection of numerical and statistical algorithms which can be embedded into your codes on ARCHER at compile time. NAMD A parallel molecular dynamics application designed to simulate large bio-molecular systems. It is developed and maintained by the University of Illinois at Urbana-Champaign. NCIPLOT NCIPLOT is a program that enables the computation and graphical visualisation of inter- and intra-molecular non-covalent interactions (hydrogen bonds, ?-? interactions, ...) in systems ranging from small dimers to large biomolecules. NWChem NWChem aims to provide its users with computational chemistry tools that are scalable both in their ability to treat large scientific computational chemistry problems efficiently, and in their use of available parallel computing resources from high-performance parallel supercomputers to conventional workstation clusters. The NWChem software can handle: Biomolecules, nanostructures, and solid-state; From quantum to classical, and all combinations; Gaussian basis functions or plane-waves; Scaling from one to thousands of processors; Properties and relativity. ONETEP ONETEP (Order-N Electronic Total Energy Package) is a linear-scaling code for quantum-mechanical calculations based on density-functional theory OpenFOAM OpenFOAM is an open-source toolbox for computational fluid dynamics. OpenFOAM consists of generic tools to simulate complex physics for a variety of fields of interest, from fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics, electromagnetism and the pricing of financial options. The core technology of OpenFOAM is a flexible set of modules written in C++. These are used to build solvers and utilities to perform pre- and post-processing tasks ranging from simple data manipulation to visualisation and mesh processing. Paraview Paraview is a data visualisation and analysis package. Whilst ARCHER compute or login nodes do not have graphics cards installed in them paraview is installed so the visualisation libraries and applications can be used to post-process simulation data. To this end the pvserver application has been installed, along with the paraview libraries and client application. PLUMED PLUMED is a plugin that works with a large number of molecular dynamics codes. It can be used to analyse features of the dynamics on-the-fly or to perform a wide variety of free energy methods. Quantum Espresso Quantum Espresso is an integrated suite of Open-Source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials. SIESTA (Spanish Initiative for Electronic Simulations with Thousands of Atoms) is a program to perform electronic structure calculations and ab initio molecular dynamics simulations of molecules and solids. SLEPc (Scalable Library for Eigenvalue Problem Computations) a library for parallel computation of eigenvalues and eigenvectors. Based on PETSc. Note: not centrally installed as a module, only build instructions provided. TINKER The TINKER molecular modeling software is a complete and general package for molecular mechanics and dynamics, with some special features for biopolymers. VASP A package for ab initio, quantum-mechanical, molecular dynamics simulations. WIEN2k WIEN2k allows to perform electronic structure calculations of solids using density functional theory. It is based on the full-potential (linearized) augmented plane-wave and local orbitals method, one among the most accurate schemes for band structure calculations. WRF WRF is the Weather Research and Forecasting model.","title":"Scientific Software Packages"},{"location":"softwares/","text":"Madhava Software Catalogue \u00b6 The software catalogue provides an automatically updated list of the software available on the system along with information on the current default versions and historical records of which versions were defaults in the past. Access the Madhava Software Catalogue Scientific Software \u00b6 The Madhava HPC includes a number of centrally installed scientific software packages. A list of this software with details of how to access it and run jobs (including example job submission scripts) is available in the Software Packages Documentation . Application Development Environment \u00b6 The application development environment includes a number of different compilers, parallel programming models, numerical/IO libraries, debuggers and profiling tools. In short: Environment Decription Compilers GNU, Intel Parallel Programming Models MPI, OpenMP, and more Libraries BLAS, LAPACK, BLACS, ScaLAPACK, FFTW, NetCDF, HDF5, and more For more information see: User Guide: Application Development Environment Best Practice Guide: Programming Environment Best Practice Guide: Performance Analysis Best Practice Guide: Debugging Job Submission System \u00b6 MAHDAHA uses the Slurm job submission system. Operating System \u00b6 The operating system is Centos 7.7","title":"Software"},{"location":"softwares/#madhava-software-catalogue","text":"The software catalogue provides an automatically updated list of the software available on the system along with information on the current default versions and historical records of which versions were defaults in the past. Access the Madhava Software Catalogue","title":"Madhava Software Catalogue"},{"location":"softwares/#scientific-software","text":"The Madhava HPC includes a number of centrally installed scientific software packages. A list of this software with details of how to access it and run jobs (including example job submission scripts) is available in the Software Packages Documentation .","title":"Scientific Software"},{"location":"softwares/#application-development-environment","text":"The application development environment includes a number of different compilers, parallel programming models, numerical/IO libraries, debuggers and profiling tools. In short: Environment Decription Compilers GNU, Intel Parallel Programming Models MPI, OpenMP, and more Libraries BLAS, LAPACK, BLACS, ScaLAPACK, FFTW, NetCDF, HDF5, and more For more information see: User Guide: Application Development Environment Best Practice Guide: Programming Environment Best Practice Guide: Performance Analysis Best Practice Guide: Debugging","title":"Application Development Environment"},{"location":"softwares/#job-submission-system","text":"MAHDAHA uses the Slurm job submission system.","title":"Job Submission System"},{"location":"softwares/#operating-system","text":"The operating system is Centos 7.7","title":"Operating System"},{"location":"softwarescatalogue/","text":"Madhava Software Catalogue \u00b6 The software catalogue provides an automatically updated list of the software available on the system along with information on the current default versions and historical records of which versions were defaults in the past.","title":"Madhava Software Catalogue"},{"location":"softwarescatalogue/#madhava-software-catalogue","text":"The software catalogue provides an automatically updated list of the software available on the system along with information on the current default versions and historical records of which versions were defaults in the past.","title":"Madhava Software Catalogue"},{"location":"storage/","text":"Storage \u00b6 About Madhava HPC Storage \u00b6 Since there are many users and large computations used to run, much more space is needed to store all user data compared to a personal computer. In Madhava HPC Cluster different types of storages are there. Like a desktop or laptop computer, individual cluster nodes often have local disk drives. Since this storage is local to a node, it is usually faster to access by the processes running on the node. On Madhava HPC Cluster these local drives are used for system softwares. Most of the data on a cluster is kept in separate storage units that have multiple hard drives. These units are called file servers. A file server is a computer with the primary purpose of providing a location to store data. Regular users do not login to file servers. On HPC clusters these file servers are connected to the same Infiniband switch that connects all nodes, providing relatively fast access to data from all cluster nodes. Every user on a cluster has a home directory. If you type \"pwd\" right after ssh-ing to a cluster, you should see /gpfs-home/ , where is your Login Id & home directories are accessed on any cluster node. Since there are multiple users on a cluster, to minimize one user's actions affecting other users, home directories have quotas. On HPC clusters listed on this site, one can not keep more than 500GB of data in their home directory. Users keep important data in the home directories. For optimal performance we recommend to keep file system less than 70% full. And finally cluster have separate scratch space named /gpfs-scratch/ that is mounted on all nodes. This storage is not backed up and is purged regularly. Users are requested to use this space as their SCRATCH directory instead of /tmp for smooth running of the program. Since many high computational jobs create scratch files of large size, using scratch to /tmp will lead to inapprpriate halting of the code and functioning of the cluster. If users couldn't find their directory in /gpfs-scratch, please contact CCMS Admin. Storage Policies \u00b6 Use /gpfs-home/ and /gpfs-scratch/ as your home and scratch directory. regularly backup your important files from /gpfs-scratch The quota for home and scratch is shown in table below. Directory Quota Home directory 204TB Scratch Directoty 70 TB User Home 500 Gb","title":"Storage"},{"location":"storage/#storage","text":"","title":"Storage"},{"location":"storage/#about-madhava-hpc-storage","text":"Since there are many users and large computations used to run, much more space is needed to store all user data compared to a personal computer. In Madhava HPC Cluster different types of storages are there. Like a desktop or laptop computer, individual cluster nodes often have local disk drives. Since this storage is local to a node, it is usually faster to access by the processes running on the node. On Madhava HPC Cluster these local drives are used for system softwares. Most of the data on a cluster is kept in separate storage units that have multiple hard drives. These units are called file servers. A file server is a computer with the primary purpose of providing a location to store data. Regular users do not login to file servers. On HPC clusters these file servers are connected to the same Infiniband switch that connects all nodes, providing relatively fast access to data from all cluster nodes. Every user on a cluster has a home directory. If you type \"pwd\" right after ssh-ing to a cluster, you should see /gpfs-home/ , where is your Login Id & home directories are accessed on any cluster node. Since there are multiple users on a cluster, to minimize one user's actions affecting other users, home directories have quotas. On HPC clusters listed on this site, one can not keep more than 500GB of data in their home directory. Users keep important data in the home directories. For optimal performance we recommend to keep file system less than 70% full. And finally cluster have separate scratch space named /gpfs-scratch/ that is mounted on all nodes. This storage is not backed up and is purged regularly. Users are requested to use this space as their SCRATCH directory instead of /tmp for smooth running of the program. Since many high computational jobs create scratch files of large size, using scratch to /tmp will lead to inapprpriate halting of the code and functioning of the cluster. If users couldn't find their directory in /gpfs-scratch, please contact CCMS Admin.","title":"About Madhava HPC Storage"},{"location":"storage/#storage-policies","text":"Use /gpfs-home/ and /gpfs-scratch/ as your home and scratch directory. regularly backup your important files from /gpfs-scratch The quota for home and scratch is shown in table below. Directory Quota Home directory 204TB Scratch Directoty 70 TB User Home 500 Gb","title":"Storage Policies"},{"location":"system_configuration/","text":"System Hardware & Configuration \u00b6 Total number of nodes: 36 (1 + 31 + 2 + 2) Master/Login/Service nodes: 1 CPU nodes: 31 GPU accelerated nodes: 2 Storage nodes: 2 Master/Login/Service Node:1 Madhava HPC cluster is an aggregation of a large number of computers connected through networks. The basic purpose of the master node is to manage and monitor each of the constituent component of Madhava HPC from a system\u2019s perspective. This involves operations like monitoring the health of the components, the load on the components, the utilization of various sub-components of the computers in Madhava HPC. This also works as a login node which used for administrative tasks such as editing, writing scripts, transferring files, managing your jobs and the like. You will always get connected to one of the login nodes. From the login nodes you can get connected to a compute node and execute and interactive job or submit batch jobs through the batch system (SLURM) to run your jobs on compute nodes. For ALL users Madhava HPC Cluster login nodes are the entry points and hence are shared. This node also works as a service node to provide Job Scheduling Services and other services to the cluster Master/ Login/service Nodes : 1 |Model |Lenovo ThinkSystem SR650 | |Processor |2 nos of Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz.| |Cores |64 cores | |RAM |192 GB | |HDD |2 TB | CPU Nodes:31 CPU nodes are indeed the work horses of Madhava HPC. All the CPU intensive activities are carried on these nodes. Users can access these nodes from the login node to run interactive or batch jobs. by users in the aforementioned way. CPU Only Nodes : 31 |Model |Lenovo ThinkSystem SR630 | |Processor |2 nos of Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz. | |Cores |40 cores | |RAM |192 GB | |HDD |1 TB | GPU Nodes:2 GPU compute nodes are the nodes that have CPU cores along with accelerators cards. For some applications GPUs get markedly high performance. For exploiting these, one has to make use of special libraries which map computations on the Graphical Processing Units (Typically one has to make use of CUDA or OpencCL). CPU Only Nodes : 31 |Model |Lenovo ThinkSystem SR650 | |Processor |2 nos of Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz. | |Cores |40 cores | |RAM |192 GB | |HDD |1 TB | |GPU |2* Tesla V100-PCIE-32GB per node | |GPU cores per node |2* 5120 | |GPU Tensor Cores |2* 640 | Storage/ IO nodes:2 Providing the storage service to head and compute nodes using GPFS Storage/ IO nodes:2 |Model |Lenovo ThinkSystem SR630 | |Processor |2 nos of Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz. | |Cores |40 cores | |RAM |192 GB | |HDD |1 TB | Storage Array :2 It's a backend to the IO nodes which is providing storage service to head and compute nodes Storage/ IO nodes:2 |Model |ThinkSystem DE6000H 2 nos of DE Controller with 16 GB, 60 Chasis. | |Home Space |204 TB | |Scratch Space |60 TB | Installing gmx_MMPBSA \u00b6 Requirements \u00b6 gmx_MMPBSA requires GROMACS (series 4.x.x or 5.x.x or 20xx.x ) and AmberTools20 or 21 to be installed in your machine with Python3 . gmx_MMPBSA has been tested with GROMACS 4.6.7 , 5.1.2 , 2018.3 , 2020.4 and 2021.3 , although it should run smoothly with any GROMACS present in the PATH and that is compatible with the files you are using. Installing gmx_MMPBSA v1.5.x \u00b6 Danger gmx_MMPBSA v1.5.x includes a number of new functionalities and parts of the code have been completely rewritten, hence is incompatible with previous versions. Currently, gmx_MMPBSA can be installed using two ways: conda environment The conda environment provides a clean and efficient way of installing gmx_MMPBSA. It also allows to have different versions of gmx_MMPBSA in isolated environments, thus reducing the possibility of incompatibility with other packages. Installation time is also less since it does not require the compilation of AmberTools or GROMACS. ( Recommended, especially if you want to keep older versions of gmx_MMPBSA ) AmberTools compilation In this way, we assume that you have AmberTools compiled on your machine and that you want to do an installation without worrying about enabling or disabling conda environments. It also involves user compilation of GROMACS, which takes considerable installation time. This way also requires installed packages to be compatible and installation errors are more frequent. Installation conda environment AmberTools compilation Miniconda Installation *.yml file pip Installing gmx_MMPBSA using a yml file. Download env.yml file // Create a new environment and use the *.yml file to install dependencies $ conda env create -n gmxMMPBSA --file env.yml // To use gmx_MMPBSA, just activate the environment $ conda activate gmxMMPBSA Copy described intructions conda env create -n gmxMMPBSA --file env.yml # (1) conda activate gmxMMPBSA # (2) Create the gmxMMPBSA environment and use the *.yml file to install dependencies Activate gmxMMPBSA environment Installing dependencies // Update conda $ conda update conda // Create a new environment and activate it $ conda create -n gmxMMPBSA python = 3 .9 -y -q $ conda activate gmxMMPBSA // Install mpi4py and AmberTools $ conda install -c conda-forge mpi4py = 3 .1.3 ambertools = 21 .12 compilers -y -q // Install updated version of ParmEd $ python -m pip install git+https://github.com/Valdes-Tresanco-MS/ParmEd.git@v3.4 // Install PyQt5 required to use the GUI analyzer tool (gmx_MMPBSA_ana). Not needed for HPC $ python -m pip install pyqt5 // (Optional) Install GROMACS $ conda install -c bioconda gromacs == 2021 .3 -y -q Copy described intructions conda update conda conda create -n gmxMMPBSA python = 3 .9 -y -q # (1) conda activate gmxMMPBSA # (2) conda install -c conda-forge mpi4py = 3 .1.3 ambertools = 21 .12 compilers -y -q # (3) python -m pip install git+https://github.com/Valdes-Tresanco-MS/ParmEd.git@v3.4 # (4) python -m pip install pyqt5 # (5) # Optional conda install -c bioconda gromacs == 2021 .3 -y -q # (6) Create gmxMMPBSA environment Activate gmxMMPBSA environment Install dependencies Install ParmEd Install PyQt5 if you will use gmx_MMPBSA_ana (Optional) Install GROMACS if GROMACS is not installed in your machine Rolling/stable release development version INSTALLATION // INSTALLATION $ python -m pip install gmx_MMPBSA UPDATE // UPDATE $ python -m pip install gmx_MMPBSA -U Info Install/update gmx_MMPBSA from PyPI. PyPI has the latest version of gmx_MMPBSA including stable and beta versions. INSTALLATION // INSTALLATION $ python -m pip install git+https://github.com/Valdes-Tresanco-MS/gmx_MMPBSA UPDATE // UPDATE $ python -m pip install git+https://github.com/Valdes-Tresanco-MS/gmx_MMPBSA -U Warning Install gmx_MMPBSA from the master branch of GitHub repository. This is only recommended for testing new versions or temporary solutions to reported bugs. Follow the oficial AmberTools installation according to your OS Note We asume that AmberTools and their shell environment are correctly configured Rolling/stable release development version INSTALLATION // Install uodated ParmEd $ amber.python -m pip install git+https://github.com/Valdes-Tresanco-MS/ParmEd.git@v3.4 // Install gmx_MMPBSA $ amber.python -m pip install gmx_MMPBSA UPDATE // Update gmx_MMPBSA $ amber.python -m pip install gmx_MMPBSA -U Info Install gmx_MMPBSA from PyPI PyPI has the latest version of gmx_MMPBSA including stable and beta versions. INSTALLATION // Install updated ParmEd $ amber.python -m pip install git+https://github.com/Valdes-Tresanco-MS/ParmEd.git@v3.4 // Install gmx_MMPBSA $ amber.python -m pip install git+https://github.com/Valdes-Tresanco-MS/gmx_MMPBSA UPDATE amber.python -m pip install git+https://github.com/Valdes-Tresanco-MS/gmx_MMPBSA -U Warning Install/update gmx_MMPBSA from the master branch of GitHub repository. This version is only recommended to test a new version or to try temporary solutions to reported bugs. Danger If you get an error related to installing mpi4py , you may want to install this package manually from conda-forge as follows: amber.conda install -c conda-forge mpi4py=3.1.3 If you get an error related to pip , you may want to install this package manually as follows: amber.conda install pip Miniconda installation $ curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh ---> 100 % $ chmod +x Miniconda3-latest-Linux-x86_64.sh $ ./Miniconda3-latest-Linux-x86_64.sh ---> 100 % Successful miniconda intallation Extra Dependencies \u00b6 gmx_MMPBSA uses some dependencies for other functions independent of calculations or in some cases they may be necessary due to the nature of your OS. ParmEd The current version of ParmEd implemented in AmberTools has some limitations that have been resolved in the GitHub repository by its author Jason Swails and others with our help. Some of these limitations are: Error reading topology when it has insertion codes Error processing topologies generated with the Amber ff19SB force field New PBRadii sets for GAFF and CHARMM force fields Danger The gmx_MMPBSA installation process has been optimized to be as straightforward as possible. In rare cases, a few extra dependencies may be needed. pip In some cases, the miniconda environment created in the AmberTools compilation does not have the pip module, so any installation that depends on this package will fail. Required only if you did not install gmx_MMPBSA via conda amber.conda install pip Git Used by gmx_MMPBSA_test to download the GitHub repository to get the examples' folder or to install the development version. conda install -c anaconda git or sudo apt install git mpi In some cases it is necessary to install the MPI dependencies. Required only if you did not install gmx_MMPBSA via conda sudo apt install openmpi-bin libopenmpi-dev openssh-client libxcb If you get an error related to Qt plugins sudo apt install --reinstall libxcb-xinerama0 Troubleshooting after installation \u00b6 Once the installation is completed, the following warning may appear: WARNING: The scripts gmx_MMPBSA, gmx_MMPBSA_ana and gmx_MMPBSA_test are installed in '/home/user/path_to_amber_install/amber20/miniconda/bin' which is not on PATH. This warning is because pip installs the executables ( gmx_MMPBSA , gmx_MMPBSA_ana and gmx_MMPBSA_test ) in installation_path/amber20/miniconda/bin . You have two options to solve this: Add this folder (*/amber20/miniconda/bin) to PATH: export PATH=\"/path_to_amber_install/amber20/miniconda/bin:$PATH\" Tip This option is more permanent and is recommended if you don't want to activate and deactivate the conda environment Make sure to update path_to_amber_install in the PATH variable Initializing the environment of conda amber: amber.conda init bash You can deactivate like this: conda deactivate Note After using one of the above options, you should be able to run gmx_MMPBSA , gmx_MMPBSA_ana and gmx_MMPBSA_test through the terminal If when running gmx_MMPBSA , you get an error like this: ModuleNotFoundError: No module named 'parmed' please see the following issue to see the solution Autocompletion script \u00b6 Since gmx_MMPBSA has many flags, we believe that this autocompletion can significantly improve productivity, be more user-friendly and reduce the number of unforced errors. That is why we created this script, which manages the autocompletion of the gmx_MMPBSA , gmx_MMPBSA_ana and gmx_MMPBSA_test . Execution: Enter the following command in the terminal: source /path/to/ambertools/lib/python3.x/site-packages/GMXMMPBSA/GMXMMPBSA.sh Tip If you want it to be activated automatically, add that command to your .bashrc Warning This script requires that gmx_MMPBSA , gmx_MMPBSA_ana and gmx_MMPBSA_test be accessible in PATH If the command-line above end in error, please make sure the file has executed permissions. On Ubuntu, Debian, Linux Mint or related: GUI: Right-click on GMXMMPBSA.sh file > Properties > Permissions > Mark the checkbox \"Allow to execute the file as a program\" Terminal: chmod 755 /path/to/ambertools/lib/python3.x/site-packages/GMXMMPBSA/GMXMMPBSA.sh Once you make the source of GMXMMPBSA.sh you can check its operation as follows: All you have to do is enter the name of the program in the terminal and press the tab key twice: gmx_MMPBSA <tab> <tab> Testing the operation of gmx_MMPBSA \u00b6 After preparing everything to run gmx_MMPBSA , it only remains to check its correct operation. To know how to do it, consult the documentation of gmx_MMPBSA_test","title":"System Configuration"},{"location":"system_configuration/#system-hardware-configuration","text":"Total number of nodes: 36 (1 + 31 + 2 + 2) Master/Login/Service nodes: 1 CPU nodes: 31 GPU accelerated nodes: 2 Storage nodes: 2 Master/Login/Service Node:1 Madhava HPC cluster is an aggregation of a large number of computers connected through networks. The basic purpose of the master node is to manage and monitor each of the constituent component of Madhava HPC from a system\u2019s perspective. This involves operations like monitoring the health of the components, the load on the components, the utilization of various sub-components of the computers in Madhava HPC. This also works as a login node which used for administrative tasks such as editing, writing scripts, transferring files, managing your jobs and the like. You will always get connected to one of the login nodes. From the login nodes you can get connected to a compute node and execute and interactive job or submit batch jobs through the batch system (SLURM) to run your jobs on compute nodes. For ALL users Madhava HPC Cluster login nodes are the entry points and hence are shared. This node also works as a service node to provide Job Scheduling Services and other services to the cluster Master/ Login/service Nodes : 1 |Model |Lenovo ThinkSystem SR650 | |Processor |2 nos of Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz.| |Cores |64 cores | |RAM |192 GB | |HDD |2 TB | CPU Nodes:31 CPU nodes are indeed the work horses of Madhava HPC. All the CPU intensive activities are carried on these nodes. Users can access these nodes from the login node to run interactive or batch jobs. by users in the aforementioned way. CPU Only Nodes : 31 |Model |Lenovo ThinkSystem SR630 | |Processor |2 nos of Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz. | |Cores |40 cores | |RAM |192 GB | |HDD |1 TB | GPU Nodes:2 GPU compute nodes are the nodes that have CPU cores along with accelerators cards. For some applications GPUs get markedly high performance. For exploiting these, one has to make use of special libraries which map computations on the Graphical Processing Units (Typically one has to make use of CUDA or OpencCL). CPU Only Nodes : 31 |Model |Lenovo ThinkSystem SR650 | |Processor |2 nos of Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz. | |Cores |40 cores | |RAM |192 GB | |HDD |1 TB | |GPU |2* Tesla V100-PCIE-32GB per node | |GPU cores per node |2* 5120 | |GPU Tensor Cores |2* 640 | Storage/ IO nodes:2 Providing the storage service to head and compute nodes using GPFS Storage/ IO nodes:2 |Model |Lenovo ThinkSystem SR630 | |Processor |2 nos of Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz. | |Cores |40 cores | |RAM |192 GB | |HDD |1 TB | Storage Array :2 It's a backend to the IO nodes which is providing storage service to head and compute nodes Storage/ IO nodes:2 |Model |ThinkSystem DE6000H 2 nos of DE Controller with 16 GB, 60 Chasis. | |Home Space |204 TB | |Scratch Space |60 TB |","title":"System Hardware &amp; Configuration"},{"location":"system_configuration/#installing-gmx_mmpbsa","text":"","title":"Installing gmx_MMPBSA"},{"location":"system_configuration/#requirements","text":"gmx_MMPBSA requires GROMACS (series 4.x.x or 5.x.x or 20xx.x ) and AmberTools20 or 21 to be installed in your machine with Python3 . gmx_MMPBSA has been tested with GROMACS 4.6.7 , 5.1.2 , 2018.3 , 2020.4 and 2021.3 , although it should run smoothly with any GROMACS present in the PATH and that is compatible with the files you are using.","title":"Requirements"},{"location":"system_configuration/#installing-gmx_mmpbsa-v15x","text":"Danger gmx_MMPBSA v1.5.x includes a number of new functionalities and parts of the code have been completely rewritten, hence is incompatible with previous versions. Currently, gmx_MMPBSA can be installed using two ways: conda environment The conda environment provides a clean and efficient way of installing gmx_MMPBSA. It also allows to have different versions of gmx_MMPBSA in isolated environments, thus reducing the possibility of incompatibility with other packages. Installation time is also less since it does not require the compilation of AmberTools or GROMACS. ( Recommended, especially if you want to keep older versions of gmx_MMPBSA ) AmberTools compilation In this way, we assume that you have AmberTools compiled on your machine and that you want to do an installation without worrying about enabling or disabling conda environments. It also involves user compilation of GROMACS, which takes considerable installation time. This way also requires installed packages to be compatible and installation errors are more frequent. Installation conda environment AmberTools compilation Miniconda Installation *.yml file pip Installing gmx_MMPBSA using a yml file. Download env.yml file // Create a new environment and use the *.yml file to install dependencies $ conda env create -n gmxMMPBSA --file env.yml // To use gmx_MMPBSA, just activate the environment $ conda activate gmxMMPBSA Copy described intructions conda env create -n gmxMMPBSA --file env.yml # (1) conda activate gmxMMPBSA # (2) Create the gmxMMPBSA environment and use the *.yml file to install dependencies Activate gmxMMPBSA environment Installing dependencies // Update conda $ conda update conda // Create a new environment and activate it $ conda create -n gmxMMPBSA python = 3 .9 -y -q $ conda activate gmxMMPBSA // Install mpi4py and AmberTools $ conda install -c conda-forge mpi4py = 3 .1.3 ambertools = 21 .12 compilers -y -q // Install updated version of ParmEd $ python -m pip install git+https://github.com/Valdes-Tresanco-MS/ParmEd.git@v3.4 // Install PyQt5 required to use the GUI analyzer tool (gmx_MMPBSA_ana). Not needed for HPC $ python -m pip install pyqt5 // (Optional) Install GROMACS $ conda install -c bioconda gromacs == 2021 .3 -y -q Copy described intructions conda update conda conda create -n gmxMMPBSA python = 3 .9 -y -q # (1) conda activate gmxMMPBSA # (2) conda install -c conda-forge mpi4py = 3 .1.3 ambertools = 21 .12 compilers -y -q # (3) python -m pip install git+https://github.com/Valdes-Tresanco-MS/ParmEd.git@v3.4 # (4) python -m pip install pyqt5 # (5) # Optional conda install -c bioconda gromacs == 2021 .3 -y -q # (6) Create gmxMMPBSA environment Activate gmxMMPBSA environment Install dependencies Install ParmEd Install PyQt5 if you will use gmx_MMPBSA_ana (Optional) Install GROMACS if GROMACS is not installed in your machine Rolling/stable release development version INSTALLATION // INSTALLATION $ python -m pip install gmx_MMPBSA UPDATE // UPDATE $ python -m pip install gmx_MMPBSA -U Info Install/update gmx_MMPBSA from PyPI. PyPI has the latest version of gmx_MMPBSA including stable and beta versions. INSTALLATION // INSTALLATION $ python -m pip install git+https://github.com/Valdes-Tresanco-MS/gmx_MMPBSA UPDATE // UPDATE $ python -m pip install git+https://github.com/Valdes-Tresanco-MS/gmx_MMPBSA -U Warning Install gmx_MMPBSA from the master branch of GitHub repository. This is only recommended for testing new versions or temporary solutions to reported bugs. Follow the oficial AmberTools installation according to your OS Note We asume that AmberTools and their shell environment are correctly configured Rolling/stable release development version INSTALLATION // Install uodated ParmEd $ amber.python -m pip install git+https://github.com/Valdes-Tresanco-MS/ParmEd.git@v3.4 // Install gmx_MMPBSA $ amber.python -m pip install gmx_MMPBSA UPDATE // Update gmx_MMPBSA $ amber.python -m pip install gmx_MMPBSA -U Info Install gmx_MMPBSA from PyPI PyPI has the latest version of gmx_MMPBSA including stable and beta versions. INSTALLATION // Install updated ParmEd $ amber.python -m pip install git+https://github.com/Valdes-Tresanco-MS/ParmEd.git@v3.4 // Install gmx_MMPBSA $ amber.python -m pip install git+https://github.com/Valdes-Tresanco-MS/gmx_MMPBSA UPDATE amber.python -m pip install git+https://github.com/Valdes-Tresanco-MS/gmx_MMPBSA -U Warning Install/update gmx_MMPBSA from the master branch of GitHub repository. This version is only recommended to test a new version or to try temporary solutions to reported bugs. Danger If you get an error related to installing mpi4py , you may want to install this package manually from conda-forge as follows: amber.conda install -c conda-forge mpi4py=3.1.3 If you get an error related to pip , you may want to install this package manually as follows: amber.conda install pip Miniconda installation $ curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh ---> 100 % $ chmod +x Miniconda3-latest-Linux-x86_64.sh $ ./Miniconda3-latest-Linux-x86_64.sh ---> 100 % Successful miniconda intallation","title":"Installing gmx_MMPBSA v1.5.x"},{"location":"system_configuration/#extra-dependencies","text":"gmx_MMPBSA uses some dependencies for other functions independent of calculations or in some cases they may be necessary due to the nature of your OS. ParmEd The current version of ParmEd implemented in AmberTools has some limitations that have been resolved in the GitHub repository by its author Jason Swails and others with our help. Some of these limitations are: Error reading topology when it has insertion codes Error processing topologies generated with the Amber ff19SB force field New PBRadii sets for GAFF and CHARMM force fields Danger The gmx_MMPBSA installation process has been optimized to be as straightforward as possible. In rare cases, a few extra dependencies may be needed. pip In some cases, the miniconda environment created in the AmberTools compilation does not have the pip module, so any installation that depends on this package will fail. Required only if you did not install gmx_MMPBSA via conda amber.conda install pip Git Used by gmx_MMPBSA_test to download the GitHub repository to get the examples' folder or to install the development version. conda install -c anaconda git or sudo apt install git mpi In some cases it is necessary to install the MPI dependencies. Required only if you did not install gmx_MMPBSA via conda sudo apt install openmpi-bin libopenmpi-dev openssh-client libxcb If you get an error related to Qt plugins sudo apt install --reinstall libxcb-xinerama0","title":"Extra Dependencies"},{"location":"system_configuration/#troubleshooting-after-installation","text":"Once the installation is completed, the following warning may appear: WARNING: The scripts gmx_MMPBSA, gmx_MMPBSA_ana and gmx_MMPBSA_test are installed in '/home/user/path_to_amber_install/amber20/miniconda/bin' which is not on PATH. This warning is because pip installs the executables ( gmx_MMPBSA , gmx_MMPBSA_ana and gmx_MMPBSA_test ) in installation_path/amber20/miniconda/bin . You have two options to solve this: Add this folder (*/amber20/miniconda/bin) to PATH: export PATH=\"/path_to_amber_install/amber20/miniconda/bin:$PATH\" Tip This option is more permanent and is recommended if you don't want to activate and deactivate the conda environment Make sure to update path_to_amber_install in the PATH variable Initializing the environment of conda amber: amber.conda init bash You can deactivate like this: conda deactivate Note After using one of the above options, you should be able to run gmx_MMPBSA , gmx_MMPBSA_ana and gmx_MMPBSA_test through the terminal If when running gmx_MMPBSA , you get an error like this: ModuleNotFoundError: No module named 'parmed' please see the following issue to see the solution","title":"Troubleshooting after installation"},{"location":"system_configuration/#autocompletion-script","text":"Since gmx_MMPBSA has many flags, we believe that this autocompletion can significantly improve productivity, be more user-friendly and reduce the number of unforced errors. That is why we created this script, which manages the autocompletion of the gmx_MMPBSA , gmx_MMPBSA_ana and gmx_MMPBSA_test . Execution: Enter the following command in the terminal: source /path/to/ambertools/lib/python3.x/site-packages/GMXMMPBSA/GMXMMPBSA.sh Tip If you want it to be activated automatically, add that command to your .bashrc Warning This script requires that gmx_MMPBSA , gmx_MMPBSA_ana and gmx_MMPBSA_test be accessible in PATH If the command-line above end in error, please make sure the file has executed permissions. On Ubuntu, Debian, Linux Mint or related: GUI: Right-click on GMXMMPBSA.sh file > Properties > Permissions > Mark the checkbox \"Allow to execute the file as a program\" Terminal: chmod 755 /path/to/ambertools/lib/python3.x/site-packages/GMXMMPBSA/GMXMMPBSA.sh Once you make the source of GMXMMPBSA.sh you can check its operation as follows: All you have to do is enter the name of the program in the terminal and press the tab key twice: gmx_MMPBSA <tab> <tab>","title":"Autocompletion script"},{"location":"system_configuration/#testing-the-operation-of-gmx_mmpbsa","text":"After preparing everything to run gmx_MMPBSA , it only remains to check its correct operation. To know how to do it, consult the documentation of gmx_MMPBSA_test","title":"Testing the operation of gmx_MMPBSA"}]}